{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "from ntscraper import Nitter\n",
    "import random\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class active_user:\n",
    "  def __init__(self, username, comments, retweets, quotes, likes):\n",
    "    self.username = username\n",
    "    self.comments = comments\n",
    "    self.retweets = retweets\n",
    "    self.quotes = quotes\n",
    "    self.likes = likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# #Select 10,000 Row from our Dataset\n",
    "#\n",
    "# def read_and_save_csv(input_file_path, output_file_path, num_rows):\n",
    "#     with open(input_file_path, 'r', newline='') as infile:\n",
    "#         reader = csv.reader(infile)\n",
    "#\n",
    "#         header = next(reader, None)\n",
    "#         rows = [next(reader) for _ in range(num_rows)]\n",
    "#\n",
    "#     with open(output_file_path, 'w', newline='') as outfile:\n",
    "#         writer = csv.writer(outfile)\n",
    "#         if header:\n",
    "#             writer.writerow(header)\n",
    "#         writer.writerows(rows)\n",
    "#\n",
    "#\n",
    "# input_file_path = 'data/data.csv'\n",
    "# output_file_path = 'data/output.csv'\n",
    "# num_rows_to_read = 10000\n",
    "#\n",
    "# read_and_save_csv(input_file_path, output_file_path, num_rows_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_file(file_path, num_rows):\n",
    "    with open(file_path, 'r', newline='') as csvfile:\n",
    "\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader, None)\n",
    "        rows = [next(reader) for _ in range(num_rows)]\n",
    "\n",
    "    return header, rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_all_freiend(rows):\n",
    "    table_friends = []\n",
    "    for row in rows:\n",
    "        list_friends = row[9:]\n",
    "        table_friends.append(list_friends)\n",
    "\n",
    "    table_friends_int = []\n",
    "    for list_friends in table_friends:\n",
    "        list_friends_int = []\n",
    "        for friends in list_friends:\n",
    "            pattern = re.compile(r'\\b\\d+\\b')\n",
    "\n",
    "            # Find all matches in the input string\n",
    "            matches = pattern.findall(friends)\n",
    "\n",
    "            # Convert the matched strings to integers\n",
    "            number = [int(match) for match in matches]\n",
    "            if not number:\n",
    "                number = -1\n",
    "            else:\n",
    "                number = number[0]\n",
    "            list_friends_int.append(number)\n",
    "        table_friends_int.append(list_friends_int)\n",
    "\n",
    "    return table_friends_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_node_edge(rows, table_friends):\n",
    "\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    real_person = []\n",
    "    first = random.randint(0, 1499)\n",
    "    while len(nodes) < 20000:\n",
    "      second = first\n",
    "      first = random.randint(0, 1499)\n",
    "      nodes.append(int(rows[first][0]))\n",
    "      nodes.append(int(rows[second][0]))\n",
    "\n",
    "      real_person.append(int(rows[first][0]))\n",
    "\n",
    "      number_friends = random.randint(len(table_friends[first])//3, len(table_friends[first])//2)\n",
    "      sample_friends = random.sample(table_friends[first], number_friends)\n",
    "\n",
    "      for friend in sample_friends:\n",
    "        nodes.append(friend)\n",
    "        edges.append(tuple([int(rows[first][0]), friend]))\n",
    "\n",
    "      edges.append(tuple([int(rows[first][0]), int(rows[second][0])]))\n",
    "    \n",
    "    return [nodes, edges, real_person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_graph(nodes, edges):\n",
    "    G = networkx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_degree_centrality(G):\n",
    "    list_degreeCentrality = networkx.degree_centrality(G)\n",
    "    series = pd.Series(list_degreeCentrality)\n",
    "    series.sort_values(axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last', ignore_index=False,\n",
    "                       key=None)\n",
    "    sortedBy_degreeCentrality = series.to_dict()\n",
    "\n",
    "    return list(sortedBy_degreeCentrality)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_closeness_centrality(G):\n",
    "    csv_file_name = 'data/closeness.csv'\n",
    "\n",
    "    with open(csv_file_name, 'w', newline='') as csv_file:\n",
    "      csv_writer = csv.writer(csv_file)\n",
    "      list_closeness = {}\n",
    "      for node in G.nodes():\n",
    "        closeness_centrality = networkx.closeness_centrality(G, u=node)\n",
    "        csv_writer.writerow([node, closeness_centrality])\n",
    "        list_closeness[node] = closeness_centrality\n",
    "\n",
    "    series = pd.Series(list_closeness)\n",
    "    series.sort_values(axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last', ignore_index=False,\n",
    "                       key=None)\n",
    "    sortedBy_closenessCentrality = series.to_dict()\n",
    "    return list(sortedBy_closenessCentrality)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_betweenness_centrality(G):\n",
    "    csv_file_name = 'data/betweenness.csv'\n",
    "\n",
    "    with open(csv_file_name, 'w', newline='') as csv_file:\n",
    "      csv_writer = csv.writer(csv_file)\n",
    "\n",
    "      counter = 0\n",
    "      betweenness_centrality = networkx.betweenness_centrality(G)\n",
    "      for key,value in betweenness_centrality.items():\n",
    "        csv_writer.writerow([key, value])\n",
    "\n",
    "    series = pd.Series(betweenness_centrality)\n",
    "    series.sort_values(axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last', ignore_index=False,\n",
    "                       key=None)\n",
    "    sortedBy_betweennessCentrality = series.to_dict()\n",
    "    return list(sortedBy_betweennessCentrality)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[629551377,\n",
       " 2883178170,\n",
       " 750314805085417472,\n",
       " 320733081,\n",
       " 30545370,\n",
       " 413415713,\n",
       " 2735059424,\n",
       " 1715482856,\n",
       " 4818001,\n",
       " 3649469655,\n",
       " 2785131499,\n",
       " 2440582820,\n",
       " 1974555133,\n",
       " 188488377,\n",
       " 4827715165,\n",
       " 3366722172,\n",
       " 829620078,\n",
       " 3649469655,\n",
       " 2627656955,\n",
       " 956211552,\n",
       " 2432688799,\n",
       " 3234000347,\n",
       " 702618325247873025,\n",
       " 11385232,\n",
       " 2289149772,\n",
       " 320733081,\n",
       " 3353209209,\n",
       " 303361610,\n",
       " 2893725407,\n",
       " 351155040,\n",
       " 4501155499,\n",
       " 4520523681,\n",
       " 2889779582,\n",
       " 1369807417,\n",
       " 2456561748,\n",
       " 4757981730,\n",
       " 2659122061,\n",
       " 1969896660,\n",
       " 1957805540,\n",
       " 556794664,\n",
       " 2949013004,\n",
       " 31497230,\n",
       " 36654652,\n",
       " 4908175840,\n",
       " 17889684,\n",
       " 1478707520,\n",
       " 36175006,\n",
       " 713775631,\n",
       " 1041011882,\n",
       " 2334096980,\n",
       " 118598357,\n",
       " 737361842893262849,\n",
       " 1689933402,\n",
       " 3076225890,\n",
       " 38703275,\n",
       " 218409513,\n",
       " 89108815,\n",
       " 7502202,\n",
       " 4884801914,\n",
       " 188936945,\n",
       " 1452928032,\n",
       " 2334096980,\n",
       " 2595864763,\n",
       " 757656087956893696,\n",
       " 107950559,\n",
       " 750200934,\n",
       " 2308619040,\n",
       " 1238427218,\n",
       " 2785131499,\n",
       " 2734789755,\n",
       " 710835616146042880]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'data/output.csv'  # Replace with the actual path to your CSV file\n",
    "num_rows_to_read = 1500\n",
    "header, rows = read_csv_file(file_path, num_rows_to_read)\n",
    "\n",
    "table_friends = find_all_freiend(rows)\n",
    "[nodes, edges, real_person] = create_node_edge(rows, table_friends)\n",
    "real_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph = create_graph(nodes, edges)\n",
    "degree_centrality_list = find_degree_centrality(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[351155040, 1478707520, 176566242, 2785131499, 4827715165]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closenesss_centrality = find_closeness_centrality(graph)\n",
    "closenesss_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[351155040, 1478707520, 1715482856, 4827715165, 118598357]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betweenness_centrality = find_betweenness_centrality(graph)\n",
    "betweenness_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_all_freiend(rows):\n",
    "    table_friends = []\n",
    "    for row in rows:\n",
    "        list_friends = row[9:]\n",
    "        table_friends.append(list_friends)\n",
    "\n",
    "    table_friends_int = []\n",
    "    for list_friends in table_friends:\n",
    "        list_friends_int = []\n",
    "        for friends in list_friends:\n",
    "            pattern = re.compile(r'\\b\\d+\\b')\n",
    "\n",
    "            # Find all matches in the input string\n",
    "            matches = pattern.findall(friends)\n",
    "\n",
    "            # Convert the matched strings to integers\n",
    "            number = [int(match) for match in matches]\n",
    "            if not number:\n",
    "                number = -1\n",
    "            else:\n",
    "                number = number[0]\n",
    "            list_friends_int.append(number)\n",
    "        table_friends_int.append(list_friends_int)\n",
    "\n",
    "    return table_friends_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_node_edge(rows, table_friends):\n",
    "\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    real_person = []\n",
    "    first = random.randint(0, 1499)\n",
    "    while len(nodes) < 20000:\n",
    "      second = first\n",
    "      first = random.randint(0, 1499)\n",
    "      nodes.append(int(rows[first][0]))\n",
    "      nodes.append(int(rows[second][0]))\n",
    "\n",
    "      real_person.append(int(rows[first][0]))\n",
    "\n",
    "      number_friends = random.randint(len(table_friends[first])//3, len(table_friends[first])//2)\n",
    "      sample_friends = random.sample(table_friends[first], number_friends)\n",
    "\n",
    "      for friend in sample_friends:\n",
    "        nodes.append(friend)\n",
    "        edges.append(tuple([int(rows[first][0]), friend]))\n",
    "\n",
    "      edges.append(tuple([int(rows[first][0]), int(rows[second][0])]))\n",
    "    \n",
    "    return [nodes, edges, real_person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_graph(nodes, edges):\n",
    "    G = networkx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_degree_centrality(G):\n",
    "    list_degreeCentrality = networkx.degree_centrality(G)\n",
    "    series = pd.Series(list_degreeCentrality)\n",
    "    series.sort_values(axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last', ignore_index=False,\n",
    "                       key=None)\n",
    "    sortedBy_degreeCentrality = series.to_dict()\n",
    "\n",
    "    return list(sortedBy_degreeCentrality)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_closeness_centrality(G):\n",
    "    csv_file_name = '/content/drive/MyDrive/socialnetwork /closeness.csv'\n",
    "\n",
    "    with open(csv_file_name, 'w', newline='') as csv_file:\n",
    "      csv_writer = csv.writer(csv_file)\n",
    "      list_closeness = {}\n",
    "      for node in G.nodes():\n",
    "        closeness_centrality = networkx.closeness_centrality(G, u=node)\n",
    "        csv_writer.writerow([node, closeness_centrality])\n",
    "        list_closeness[node] = closeness_centrality\n",
    "\n",
    "    series = pd.Series(list_closeness)\n",
    "    series.sort_values(axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last', ignore_index=False,\n",
    "                       key=None)\n",
    "    sortedBy_closenessCentrality = series.to_dict()\n",
    "    return list(sortedBy_closenessCentrality)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_betweenness_centrality(G):\n",
    "    csv_file_name = '/content/drive/MyDrive/socialnetwork /betweenness.csv'\n",
    "\n",
    "    with open(csv_file_name, 'w', newline='') as csv_file:\n",
    "      csv_writer = csv.writer(csv_file)\n",
    "\n",
    "      counter = 0\n",
    "      betweenness_centrality = networkx.betweenness_centrality(G)\n",
    "      for key,value in betweenness_centrality.items():\n",
    "        csv_writer.writerow([key, value])\n",
    "\n",
    "    series = pd.Series(betweenness_centrality)\n",
    "    series.sort_values(axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last', ignore_index=False,\n",
    "                       key=None)\n",
    "    sortedBy_betweennessCentrality = series.to_dict()\n",
    "    return list(sortedBy_betweennessCentrality)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_row(file_path, person):\n",
    "  with open(file_path, newline='') as csvfile:\n",
    "    file = csv.DictReader(csvfile)\n",
    "    row = [row for row in file if row['id'] == str(person)]\n",
    "  return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_path = '/content/drive/MyDrive/socialnetwork /output.csv'  # Replace with the actual path to your CSV file\n",
    "num_rows_to_read = 1500\n",
    "\n",
    "header, rows = read_csv_file(file_path, num_rows_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "table_friends = find_all_freiend(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[nodes, edges, real_person] = create_node_edge(rows, table_friends)\n",
    "print(len(real_person))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph = create_graph(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "degree_centrality_list = find_degree_centrality(graph)\n",
    "print(degree_centrality_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "closenesss_centrality = find_closeness_centrality(graph)\n",
    "print(closenesss_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "betweenness_centrality = find_betweenness_centrality(graph)\n",
    "print(betweenness_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_voteRank = networkx.voterank(graph, number_of_nodes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "screenName = []\n",
    "list_voteRank_row = []\n",
    "for influ in list_voteRank:\n",
    "  row = find_row(file_path, influ)\n",
    "  screenName.append(row[0][\"screenName\"])\n",
    "\n",
    "print(screenName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scraper = Nitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Find 50 recent tweets of influenal people in network\n",
    "user_recent_tweets = {}\n",
    "for user in screenName:\n",
    "  try:\n",
    "    tweets = scraper.get_tweets(user, mode='user', number=50)\n",
    "    if tweets['tweets']:\n",
    "      user_recent_tweets[user] = tweets\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "json_file_path = '/content/drive/MyDrive/socialnetwork /usertweets.json'\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(user_recent_tweets, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "json_file_path = '/content/drive/MyDrive/socialnetwork /usertweets.json'\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "list_active_user = []\n",
    "for key, value in data.items():\n",
    "  comments = 0\n",
    "  retweets = 0\n",
    "  quotes = 0\n",
    "  likes = 0\n",
    "  for tweet in data[key]['tweets']:\n",
    "    state = (tweet['stats'])\n",
    "    comments += state['comments']\n",
    "    retweets += state['retweets']\n",
    "    quotes += state['quotes']\n",
    "    likes += state['likes']\n",
    "  list_active_user.append(active_user(username=key, comments=comments, retweets=retweets, quotes=quotes, likes=likes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_list_active_user = sorted(list_active_user, key=lambda x: (x.retweets, x.quotes, x.comments, x.likes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_retweets = sum(getattr(item, 'retweets') for item in sorted_real_people)\n",
    "total_quotes = sum(getattr(item, 'quotes') for item in sorted_real_people)\n",
    "total_comments = sum(getattr(item, 'comments') for item in sorted_real_people)\n",
    "total_likes = sum(getattr(item, 'likes') for item in sorted_real_people)\n",
    "\n",
    "other_retweets = sum(getattr(item, 'retweets') for item in sorted_real_people[15:])\n",
    "other_quotes = sum(getattr(item, 'quotes') for item in sorted_real_people[15:])\n",
    "other_comments = sum(getattr(item, 'comments') for item in sorted_real_people[15:])\n",
    "other_likes = sum(getattr(item, 'likes') for item in sorted_real_people[15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "active_user_content_info_path = '/content/drive/MyDrive/socialnetwork /active_user_content_info.csv'\n",
    "\n",
    "with open(active_user_content_info_path, 'w', newline='') as csvfile:\n",
    "    \n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['username', 'retweets', 'quotes', 'comments', 'likes', 'retweets-percent', 'quotes-percent', 'comments-percent', 'likes-percent'])\n",
    "    \n",
    "    for i in range(15):\n",
    "      active = sorted_real_people[i]\n",
    "      csv_writer.writerow([active.username, active.retweets, active.quotes, active.comments, active.likes, (active.retweets/total_retweets)*100, (active.quotes/total_quotes)*100, (active.comments/total_comments)*100, (active.likes/total_likes)*100])\n",
    "    \n",
    "    csv_writer.writerow([\"others\", other_retweets, other_quotes, other_comments, other_likes, (other_retweets/total_retweets)*100, (other_quotes/total_quotes)*100, (other_comments/total_comments)*100, (other_likes/total_likes)*100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "active_user_number_content_info_path = '/content/drive/MyDrive/socialnetwork /active_user_number_content_info.csv'\n",
    "\n",
    "with open(active_user_number_content_info_path, 'w', newline='') as csvfile:\n",
    "\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['username', 'num_tweets'])\n",
    "\n",
    "    for i in range(5):\n",
    "      active = sorted_real_people[i]\n",
    "      info = scraper.get_profile_info(username=active.username)\n",
    "      print(info['stats']['tweets'])\n",
    "      csv_writer.writerow([active.username, info['stats']['tweets']])\n",
    "      \n",
    "    for i in range(5, 10):\n",
    "      active = sorted_real_people[i]\n",
    "      info = scraper.get_profile_info(username=active.username)\n",
    "      print(info['stats']['tweets'])\n",
    "      csv_writer.writerow([active.username, info['stats']['tweets']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_network_node_name = []\n",
    "for n in real_person:\n",
    "  row = find_row(file_path, n)\n",
    "  print(row)\n",
    "  if row:\n",
    "    all_network_node_name.append(row[0][\"screenName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_user_recent_tweets = {}\n",
    "for user in all_network_node_name:\n",
    "  try:\n",
    "    tweets = scraper.get_tweets(user, mode='user', number=50)\n",
    "    if tweets['tweets']:\n",
    "      all_user_recent_tweets[user] = tweets\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "json_file_path_all = '/content/drive/MyDrive/socialnetwork /all_usertweets.json'\n",
    "with open(json_file_path_all, 'w') as json_file:\n",
    "    json.dump(all_user_recent_tweets, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets_date = []\n",
    "json_file_path_all = '/content/drive/MyDrive/socialnetwork /all_usertweets.json'\n",
    "\n",
    "with open(json_file_path_all, 'r') as json_file:\n",
    "    data_all_users = json.load(json_file)\n",
    "for key, value in data_all_users.items():\n",
    "  for tweet in data_all_users[key]['tweets']:\n",
    "    date = (tweet['date'])\n",
    "    tweets_date.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Convert tweet date strings to datetime objects\n",
    "tweet_date_objects = [datetime.strptime(date_str, '%b %d, %Y \\u00b7 %I:%M %p UTC') for date_str in tweets_date]\n",
    "\n",
    "daily_groups = {}\n",
    "for date_obj in tweet_date_objects:\n",
    "    day_key = date_obj.date()\n",
    "\n",
    "    if day_key not in daily_groups:\n",
    "        daily_groups[day_key] = []\n",
    "    daily_groups[day_key].append(date_obj)\n",
    "\n",
    "# Count the amount of content published daily\n",
    "content_per_day = {day: len(date_group) for day, date_group in daily_groups.items()}\n",
    "\n",
    "# Calculate the average content count per day\n",
    "total_content_count = sum(content_per_day.values())\n",
    "average_content_count = total_content_count / len(content_per_day)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total Content Published: {total_content_count}\")\n",
    "print(f\"Days: {len(content_per_day)}\")\n",
    "print(f\"Average Content Published per Day: {average_content_count}\")\n",
    "\n",
    "day_list = []\n",
    "content_count_list = []\n",
    "for day, content_count in content_per_day.items():\n",
    "  day_list.append(day)\n",
    "  content_count_list.append(content_count)\n",
    "dict_content_date = {}\n",
    "dict_content_date['day'] = day_list\n",
    "dict_content_date['count'] = content_count_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_content_date = pd.DataFrame(dict_content_date, columns=['day','count'])\n",
    "file_content_date.to_csv('/content/drive/MyDrive/socialnetwork /day_content.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Convert tweet date strings to datetime objects\n",
    "tweet_date_objects = [datetime.strptime(date_str, '%b %d, %Y \\u00b7 %I:%M %p UTC') for date_str in tweets_date]\n",
    "weekdays_dic = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "\n",
    "# Group datetime objects by the day of the week\n",
    "day_of_week_groups = {i: [] for i in range(7)}  # Initialize a dictionary for each day of the week\n",
    "for date_obj in tweet_date_objects:\n",
    "    day_of_week = date_obj.weekday()  # Monday is 0 and Sunday is 6\n",
    "    day_of_week_groups[day_of_week].append(date_obj)\n",
    "\n",
    "# Count the amount of content published for each day of the week\n",
    "content_per_day_of_week = {day_of_week: len(date_group) for day_of_week, date_group in day_of_week_groups.items()}\n",
    "\n",
    "# Print the result\n",
    "for day_of_week, content_count in content_per_day_of_week.items():\n",
    "    print(f\"Day of the Week: {weekdays_dic[day_of_week]}, Content Published: {content_count}\")\n",
    "\n",
    "numberof_content_each_weekday = '/content/drive/MyDrive/socialnetwork /numberof_content_each_weekday.csv'\n",
    "\n",
    "# Writing the dictionary to a CSV file\n",
    "with open(numberof_content_each_weekday, 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['weekday', 'numberofContent'])\n",
    "    for day_of_week, content_count in content_per_day_of_week.items():\n",
    "        csv_writer.writerow([weekdays_dic[day_of_week], content_count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Convert tweet date strings to datetime objects\n",
    "tweet_date_objects = [datetime.strptime(date_str, '%b %d, %Y \\u00b7 %I:%M %p UTC') for date_str in tweets_date]\n",
    "\n",
    "# Count the occurrences of each hour and AM/PM combination\n",
    "hour_am_pm_counts = Counter((date_obj.hour, date_obj.strftime('%p')) for date_obj in tweet_date_objects)\n",
    "\n",
    "# Find the top 5 hours with the most repetition\n",
    "top_hours = hour_am_pm_counts.most_common(5)\n",
    "\n",
    "high_traffic_hours = '/content/drive/MyDrive/socialnetwork /high_traffic_hours.csv'\n",
    "# Print the result\n",
    "for (hour, am_pm), frequency in top_hours:\n",
    "    print(f\"Hour: {hour} {am_pm}, Frequency: {frequency}\")\n",
    "\n",
    "with open(high_traffic_hours, 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Hour', 'PM/AM', 'Frequency'])\n",
    "    for (hour, am_pm), frequency in top_hours:\n",
    "      csv_writer.writerow([hour, am_pm, frequency])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "clustening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_network_node_name = []\n",
    "for n in real_person:\n",
    "  row = find_row(file_path, n)\n",
    "  print(row)\n",
    "  if row:\n",
    "    all_network_node_name.append(row[0][\"screenName\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_username = []\n",
    "list_name = []\n",
    "list_bio = []\n",
    "list_joined_date = []\n",
    "list_location = []\n",
    "for user in all_network_node_name:\n",
    "  try:\n",
    "    info = scraper.get_profile_info(user)\n",
    "    list_username.append(info['username'])\n",
    "    list_name.append(info['name'])\n",
    "    list_bio.append(info['bio'])\n",
    "    list_joined_date.append(info['joined'])\n",
    "    list_location.append(info['location'])\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_obj = {}\n",
    "user_obj['username'] = list_username\n",
    "user_obj['name'] = list_name\n",
    "user_obj['bio'] = list_bio\n",
    "user_obj['joined'] = list_joined_date\n",
    "user_obj['location'] = list_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_profile_info_frame = pd.DataFrame(user_obj, columns=['username','name', 'bio', 'joined', 'location'])\n",
    "user_profile_info_frame.to_csv('/content/drive/MyDrive/socialnetwork /profile_info_real_persons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_network_node_name = []\n",
    "all_network_node_tags = []\n",
    "for n in real_person:\n",
    "  row = find_row(file_path, n)\n",
    "  row_first = row[0]\n",
    "  if row:\n",
    "    all_network_node_tags.append(row_first['tags'][3:-3])\n",
    "    all_network_node_name.append(row[0][\"screenName\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj_tags_users = {'username': all_network_node_name, 'tags': all_network_node_tags}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_profile_tags_frame = pd.DataFrame(obj_tags_users, columns=['username','tags'])\n",
    "user_profile_tags_frame.to_csv('/content/drive/MyDrive/socialnetwork /tags_info_real_persons.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_network_node_name = []\n",
    "all_network_node_tags = []\n",
    "for n in real_person:\n",
    "  row = find_row(file_path, n)\n",
    "  row_first = row[0]\n",
    "  if row:\n",
    "    all_network_node_tags.append(row_first['tags'][3:-3])\n",
    "    all_network_node_name.append(row[0][\"screenName\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "obj_tags_users = {'username': all_network_node_name, 'tags': all_network_node_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_profile_tags_frame = pd.DataFrame(obj_tags_users, columns=['username','tags'])\n",
    "user_profile_tags_frame.to_csv('/content/drive/MyDrive/socialnetwork /tags_info_real_persons.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}